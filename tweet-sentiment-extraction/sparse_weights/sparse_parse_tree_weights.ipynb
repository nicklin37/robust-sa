{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c408f49b-b6b9-42c8-8270-6942809ea611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank \n",
    "from nltk.tree import Tree\n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a3ff3b-55d9-4e60-a93b-4e7c0ea6bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'../data/train_final.csv')\n",
    "df = df.loc[df['sentiment'] != 'neutral']\n",
    "\n",
    "def calculate_class(x):\n",
    "    if x == 'negative':\n",
    "        return 0.0\n",
    "    if x == 'positive':\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "df['classification'] = df['sentiment'].apply(lambda x : calculate_class(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ac38c91-f4c1-42ad-bf6a-b2244ced0484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  classification\n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative             0.0\n",
       "2                          my boss is bullying me...  negative             0.0\n",
       "3                     what interview! leave me alone  negative             0.0\n",
       "4   Sons of ****, why couldn`t they put them on t...  negative             0.0\n",
       "6  2am feedings for the baby are fun when he is a...  positive             1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3434797b-ed68-43ea-a4c3-a784ce429c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sooo', 'SAD', 'I', 'will', 'miss', 'you', 'here', 'in', 'San', 'Diego', '!', '!', '!']\n",
      "[('Sooo', 'NNP'), ('SAD', 'NNP'), ('I', 'PRP'), ('will', 'MD'), ('miss', 'VB'), ('you', 'PRP'), ('here', 'RB'), ('in', 'IN'), ('San', 'NNP'), ('Diego', 'NNP'), ('!', '.'), ('!', '.'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = df['text'].iloc[0]\n",
    "tokens = nltk.word_tokenize(sample_sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307f6c69-f554-4910-80bf-cd205119c0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAATAElEQVR4nO3df7BfdX3n8edLAipYIciVlQQbVum6USs/7iDKbqc1HUTbikvR4koJlBnWWXSr3W4Xd3Yaf9HqaMtaXW1ZQYNlC0h1ja6rzYJ0WkfARFhCQDQLKmRRIgmotWqD7/3jfK58gSSfb8r95t7kPh8z37mf8zmfc87nmzn3vnJ+fU6qCkmSduUJc90BSdL8Z1hIkroMC0lSl2EhSeoyLCRJXYvmugOTcNhhh9WyZcvmuhuStFdZv379d6pqakfz9smwWLZsGevWrZvrbkjSXiXJN3Y2z9NQkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkrn3yCW5pX/bNtz1/rrugeeiZv79houv3yEKS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSuiYaFknelGRjkluT/EWSJyU5KskNSTYluTLJAa3tE9v0pjZ/2ch63tzq70jy0kn2WZL0WBMLiyRLgH8HTFfV84D9gDOAdwEXVdWzgW3AuW2Rc4Ftrf6i1o4ky9tyzwVOAT6QZL9J9VuS9FiTPg21CHhykkXAgcC9wEuAq9v81cArW/nUNk2bvyJJWv0VVfWjqroL2AScMOF+S5JGTCwsqmoz8B7gmwwh8SCwHnigqra3ZvcAS1p5CXB3W3Z7a/+00fodLPNTSc5Lsi7Jui1btsz+F5KkBWySp6EWMxwVHAUcARzEcBppIqrq4qqarqrpqampSW1GkhakSZ6G+mXgrqraUlX/AHwcOAk4pJ2WAlgKbG7lzcCRAG3+wcD9o/U7WEaStAdMMiy+CZyY5MB27WEFcBvweeD01mYl8MlWXtOmafOvrapq9We0u6WOAo4GbpxgvyVJjzKxN+VV1Q1Jrga+DGwHbgIuBv4ncEWSd7S6S9oilwAfTbIJ2MpwBxRVtTHJVQxBsx04v6oemlS/JUmPNdHXqlbVKmDVo6rvZAd3M1XVD4FX7WQ9FwIXznoHd+H4/3DZntyc9hLr333WXHdBmhM+wS1J6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6JhoWSQ5JcnWSryS5PcmLkhyaZG2Sr7Wfi1vbJPmTJJuS3JLkuJH1rGztv5Zk5ST7LEl6rEkfWbwX+GxVPQd4AXA7cAFwTVUdDVzTpgFeBhzdPucBHwRIciiwCnghcAKwaiZgJEl7xsTCIsnBwC8AlwBU1Y+r6gHgVGB1a7YaeGUrnwpcVoPrgUOSPAN4KbC2qrZW1TZgLXDKpPotSXqsSR5ZHAVsAT6c5KYkH0pyEHB4Vd3b2nwLOLyVlwB3jyx/T6vbWf0jJDkvybok67Zs2TLLX0WSFrZJhsUi4Djgg1V1LPB3PHzKCYCqKqBmY2NVdXFVTVfV9NTU1GysUpLUTDIs7gHuqaob2vTVDOHx7XZ6ifbzvjZ/M3DkyPJLW93O6iVJe8jEwqKqvgXcneSftaoVwG3AGmDmjqaVwCdbeQ1wVrsr6kTgwXa66nPAyUkWtwvbJ7c6SdIesmjC638DcHmSA4A7gXMYAuqqJOcC3wBe3dp+Bng5sAn4QWtLVW1N8nbgS63d26pq64T7LUkaMdGwqKqbgekdzFqxg7YFnL+T9VwKXDqrnZMkjc0nuCVJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1jRUWSa4Zp06StG9atKuZSZ4EHAgclmQxkDbrqcCSCfdNkjRP7DIsgH8DvBE4AljPw2HxXeD9k+uWJGk+2WVYVNV7gfcmeUNVvW8P9UmSNM/0jiwAqKr3JXkxsGx0maq6bEL9kiTNI2OFRZKPAs8CbgYeatUFGBaStACMFRbANLC8qmqSnZEkzU/jPmdxK/BPJtkRSdL8Ne6RxWHAbUluBH40U1lVr5hIryRJ88q4YfGWSXZCkjS/jXs31F9PuiOSpPlr3Luhvsdw9xPAAcD+wN9V1VMn1TFJ0vwx7pHFz8yUkwQ4FThxUp2SJM0vuz3qbA3+B/DS2e+OJGk+Gvc01Gkjk09geO7ihxPpkSRp3hn3bqhfGylvB77OcCpKkrQAjHvN4pxJd0SSNH+N+/KjpUk+keS+9vnLJEsn3TlJ0vww7gXuDwNrGN5rcQTwqVYnSVoAxg2Lqar6cFVtb5+PAFMT7JckaR4ZNyzuT3Jmkv3a50zg/nEWbO1vSvLpNn1UkhuSbEpyZZIDWv0T2/SmNn/ZyDre3OrvSOItu5K0h40bFr8FvBr4FnAvcDpw9pjL/jZw+8j0u4CLqurZwDbg3FZ/LrCt1V/U2pFkOXAG8FzgFOADSfYbc9uSpFkwbli8DVhZVVNV9XSG8Hhrb6F2EfxXgA+16QAvAa5uTVYDr2zlU9s0bf6KkafFr6iqH1XVXcAm4IQx+y1JmgXjhsXPV9W2mYmq2gocO8Zy/wX4PeAnbfppwANVtb1N3wMsaeUlwN1t/duBB1v7n9bvYJmfSnJeknVJ1m3ZsmXMryVJGse4YfGEJItnJpIcSucZjSS/CtxXVesfR//GVlUXV9V0VU1PTXntXZJm07hPcP8R8MUkH2vTrwIu7CxzEvCKJC8HngQ8FXgvcEiSRe3oYSmwubXfDBwJ3JNkEXAww0X0mfoZo8tIkvaAsY4squoy4DTg2+1zWlV9tLPMm6tqaVUtY7hAfW1VvRb4PMMFcoCVwCdbeU2bps2/tr3zew1wRrtb6ijgaODGMb+fJGkWjHtkQVXdBtw2C9v8j8AVSd4B3ARc0uovAT6aZBOwlSFgqKqNSa5q294OnF9VD81CPyRJYxo7LB6PqroOuK6V72QHdzNV1Q8ZTm/taPkL6Z/2kiRNyG6/z0KStPAYFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXRMLiyRHJvl8ktuSbEzy263+0CRrk3yt/Vzc6pPkT5JsSnJLkuNG1rWytf9akpWT6rMkaccmeWSxHfj3VbUcOBE4P8ly4ALgmqo6GrimTQO8DDi6fc4DPghDuACrgBcCJwCrZgJGkrRnTCwsqureqvpyK38PuB1YApwKrG7NVgOvbOVTgctqcD1wSJJnAC8F1lbV1qraBqwFTplUvyVJj7VHrlkkWQYcC9wAHF5V97ZZ3wIOb+UlwN0ji93T6nZWL0naQyYeFkmeAvwl8Maq+u7ovKoqoGZpO+clWZdk3ZYtW2ZjlZKkZqJhkWR/hqC4vKo+3qq/3U4v0X7e1+o3A0eOLL601e2s/hGq6uKqmq6q6ampqdn9IpK0wE3ybqgAlwC3V9Ufj8xaA8zc0bQS+ORI/VntrqgTgQfb6arPAScnWdwubJ/c6iRJe8iiCa77JOA3gQ1Jbm51/wl4J3BVknOBbwCvbvM+A7wc2AT8ADgHoKq2Jnk78KXW7m1VtXWC/ZYkPcrEwqKq/hbITmav2EH7As7fybouBS6dvd5JknaHT3BLkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktS114RFklOS3JFkU5IL5ro/krSQ7BVhkWQ/4L8CLwOWA69JsnxueyVJC8deERbACcCmqrqzqn4MXAGcOsd9kqQFY9Fcd2BMS4C7R6bvAV442iDJecB5bfL7Se7YQ31bCA4DvjPXnZgP8p6Vc90FPZL75oxVmY21/OzOZuwtYdFVVRcDF891P/ZFSdZV1fRc90N6NPfNPWdvOQ21GThyZHppq5Mk7QF7S1h8CTg6yVFJDgDOANbMcZ8kacHYK05DVdX2JK8HPgfsB1xaVRvnuFsLiaf3NF+5b+4hqaq57oMkaZ7bW05DSZLmkGEhSeoyLLRbkhyS5N+OTB+R5Oq57JMWpiSvS3JWK5+d5IiReR9ylIfZ5TUL7ZYky4BPV9Xz5rov0owk1wG/W1Xr5rov+yqPLPYxSZYluT3Jf0uyMclfJXlykmcl+WyS9Un+JslzWvtnJbk+yYYk70jy/Vb/lCTXJPlymzczvMo7gWcluTnJu9v2bm3LXJ/kuSN9uS7JdJKDklya5MYkN42sSwtU22++kuTytr9eneTAJCvaPrKh7TNPbO3fmeS2JLckeU+re0uS301yOjANXN72yyeP7HuvS/Luke2eneT9rXxm2ydvTvJnbQw67UxV+dmHPsAyYDtwTJu+CjgTuAY4utW9ELi2lT8NvKaVXwd8v5UXAU9t5cOATUDa+m991PZubeU3AW9t5WcAd7TyHwBntvIhwFeBg+b638rPnO+nBZzUpi8F/jPDsD4/1+ouA94IPA24g4fPhBzSfr6F4WgC4DpgemT91zEEyBTDuHIz9f8L+BfAPwc+Bezf6j8AnDXX/y7z+eORxb7prqq6uZXXM/xivhj4WJKbgT9j+GMO8CLgY63830fWEeAPktwC/G+G8bkO72z3KuD0Vn41MHMt42Tggrbt64AnAc/cva+kfdDdVfWFVv5zYAXDvvvVVrca+AXgQeCHwCVJTgN+MO4GqmoLcGeSE5M8DXgO8IW2reOBL7X9cgXwTx//V9p37RUP5Wm3/Wik/BDDH/kHquqY3VjHaxn+V3Z8Vf1Dkq8z/JHfqaranOT+JD8P/AbDkQoMwfPrVeXgjhr16AumDzAcRTyy0fBQ7gkMf9BPB14PvGQ3tnMFw39evgJ8oqoqSYDVVfXmf0zHFyKPLBaG7wJ3JXkVQAYvaPOuB369lc8YWeZg4L4WFL/Ew6NRfg/4mV1s60rg94CDq+qWVvc54A3tF5Qkxz7eL6R9wjOTvKiV/zWwDliW5Nmt7jeBv07yFIb96TMMpzpf8NhV7XK//ATDKw1ewxAcMJyWPT3J0wGSHJpkpyOuyrBYSF4LnJvk/wAbefh9IG8Efqedbno2wyE/wOXAdJINwFkM/yujqu4HvpDk1tELhyOuZgidq0bq3g7sD9ySZGOblu4Azk9yO7AYuAg4h+F06QbgJ8CfMoTAp9s++rfA7+xgXR8B/nTmAvfojKraBtwO/GxV3djqbmO4RvJXbb1refjUrHbAW2cXuCQHAn/fDs3PYLjY7d1Kmihvwd77eM1CxwPvb6eIHgB+a267I2k+8shCktTlNQtJUpdhIUnqMiwkSV2GhTTLkhyT5OUj069IcsGEt/mLSV48yW1oYTMspNl3DPDTsKiqNVX1zglv8xcZhnSRJsK7oaQRSQ5ieKBwKcP73t/OMIjiHwNPAb4DnF1V97ZhsW8AfolhgMRz2/Qm4MnAZuAPW3m6ql6f5CPA3wPHAk9nuFX5LIYxum6oqrNbP04G3go8Efi/wDlV9f027Mpq4NcYHnR8FcO4SdczDO2yBXhDVf3NBP55tIB5ZCE90inA/6uqF7QHxj4LvA84vaqOZxgd9cKR9ouq6gSGJ+FXVdWPgd8HrqyqY6rqyh1sYzFDOLwJWMPw5PJzgee3U1iHMTxd/MtVdRzDMBijTy1/p9V/kGHU1a8zPOl8UdumQaFZ50N50iNtAP4oybsYhm/fBjwPWNuGttoPuHek/cfbz5nRfcfxqfbE/Abg21W1AaANhbKM4ahmOcOwKgAHAF/cyTZP243vJv2jGRbSiKr6apLjGK45vAO4FthYVS/aySIzI/w+xPi/TzPL/IRHjhD8k7aOh4C1VfWaWdym9Lh4GkoakeE9zj+oqj8H3s3woqipmdFRk+w/+jbAneiNzNtzPXDSzOir7U2DPzfhbUq7ZFhIj/R84Mb2QpxVDNcfTgfe1UbsvZn+XUefB5a3EVB/Y3c70F7YczbwF21E1C8yvLRnVz4F/Ku2zX+5u9uUerwbSpLU5ZGFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnq+v/DWLUE7upg9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'sentiment', data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728353ea-36d2-48fd-992a-413ab7039c20",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02406cd-4525-468c-9ea3-f6f04c66c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ad14e4b-2046-4055-9821-82f86ebd02f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>classification</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[boss, bullying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[sons, couldnt, put, releases, already, bought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[feedings, baby, fun, smiles, coos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[journey, wow, u, became, cooler, hehe, possible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[really, really, like, song, love, story, tayl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[sharpie, running, dangerously, low, ink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[want, go, music, tonight, lost, voice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[uh, oh, sunburned]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "1       Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "2                           my boss is bullying me...  negative   \n",
       "3                      what interview! leave me alone  negative   \n",
       "4    Sons of ****, why couldn`t they put them on t...  negative   \n",
       "6   2am feedings for the baby are fun when he is a...  positive   \n",
       "9    Journey!? Wow... u just became cooler.  hehe....  positive   \n",
       "11  I really really like the song Love Story by Ta...  positive   \n",
       "12       My Sharpie is running DANGERously low on ink  negative   \n",
       "13  i want to go to music tonight but i lost my vo...  negative   \n",
       "15                              Uh oh, I am sunburned  negative   \n",
       "\n",
       "    classification                                         clean_text  \n",
       "1              0.0                      [sooo, sad, miss, san, diego]  \n",
       "2              0.0                                   [boss, bullying]  \n",
       "3              0.0                          [interview, leave, alone]  \n",
       "4              0.0    [sons, couldnt, put, releases, already, bought]  \n",
       "6              1.0                [feedings, baby, fun, smiles, coos]  \n",
       "9              1.0  [journey, wow, u, became, cooler, hehe, possible]  \n",
       "11             1.0  [really, really, like, song, love, story, tayl...  \n",
       "12             0.0          [sharpie, running, dangerously, low, ink]  \n",
       "13             0.0            [want, go, music, tonight, lost, voice]  \n",
       "15             0.0                                [uh, oh, sunburned]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = text.strip()\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(lambda x: remove_stopwords(tokenization(remove_punct(x).lower())))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da86db3-ad84-451b-862a-7146cd781ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>classification</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[bos, bullying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[son, couldnt, put, release, already, bought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[feeding, baby, fun, smile, coo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[journey, wow, u, became, cooler, hehe, possible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[really, really, like, song, love, story, tayl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[sharpie, running, dangerously, low, ink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[want, go, music, tonight, lost, voice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[uh, oh, sunburned]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "1       Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "2                           my boss is bullying me...  negative   \n",
       "3                      what interview! leave me alone  negative   \n",
       "4    Sons of ****, why couldn`t they put them on t...  negative   \n",
       "6   2am feedings for the baby are fun when he is a...  positive   \n",
       "9    Journey!? Wow... u just became cooler.  hehe....  positive   \n",
       "11  I really really like the song Love Story by Ta...  positive   \n",
       "12       My Sharpie is running DANGERously low on ink  negative   \n",
       "13  i want to go to music tonight but i lost my vo...  negative   \n",
       "15                              Uh oh, I am sunburned  negative   \n",
       "\n",
       "    classification                                         clean_text  \n",
       "1              0.0                      [sooo, sad, miss, san, diego]  \n",
       "2              0.0                                    [bos, bullying]  \n",
       "3              0.0                          [interview, leave, alone]  \n",
       "4              0.0      [son, couldnt, put, release, already, bought]  \n",
       "6              1.0                   [feeding, baby, fun, smile, coo]  \n",
       "9              1.0  [journey, wow, u, became, cooler, hehe, possible]  \n",
       "11             1.0  [really, really, like, song, love, story, tayl...  \n",
       "12             0.0          [sharpie, running, dangerously, low, ink]  \n",
       "13             0.0            [want, go, music, tonight, lost, voice]  \n",
       "15             0.0                                [uh, oh, sunburned]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x: lemmatizer(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f477f80a-9bcc-4ff6-97d5-5d5a50cd0d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['clean_text'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bcf7872-7359-44f2-8e8a-0b3e6727cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c16f7979-e373-4914-a8ae-de69c030770c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>classification</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bos bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>son couldnt put release already bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>feeding baby fun smile coo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>journey wow u became cooler hehe possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>really really like song love story taylor swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sharpie running dangerously low ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>want go music tonight lost voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>uh oh sunburned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "1       Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "2                           my boss is bullying me...  negative   \n",
       "3                      what interview! leave me alone  negative   \n",
       "4    Sons of ****, why couldn`t they put them on t...  negative   \n",
       "6   2am feedings for the baby are fun when he is a...  positive   \n",
       "9    Journey!? Wow... u just became cooler.  hehe....  positive   \n",
       "11  I really really like the song Love Story by Ta...  positive   \n",
       "12       My Sharpie is running DANGERously low on ink  negative   \n",
       "13  i want to go to music tonight but i lost my vo...  negative   \n",
       "15                              Uh oh, I am sunburned  negative   \n",
       "\n",
       "    classification                                       clean_text  \n",
       "1              0.0                          sooo sad miss san diego  \n",
       "2              0.0                                     bos bullying  \n",
       "3              0.0                            interview leave alone  \n",
       "4              0.0           son couldnt put release already bought  \n",
       "6              1.0                       feeding baby fun smile coo  \n",
       "9              1.0        journey wow u became cooler hehe possible  \n",
       "11             1.0  really really like song love story taylor swift  \n",
       "12             0.0              sharpie running dangerously low ink  \n",
       "13             0.0                 want go music tonight lost voice  \n",
       "15             0.0                                  uh oh sunburned  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4fad8f6-80bb-4f6c-b5fc-55450d114d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/adversarial_swap_train_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf35c11-1434-4566-ab73-ea23bd239235",
   "metadata": {},
   "source": [
    "### Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5637c81c-3f14-480d-88ad-b40fc5b7915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16044\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for tokens in df['further_clean_text']:\n",
    "    unique_words = unique_words.union(set(tokens))\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1333098c-0796-4ddf-a96a-2e52d3817c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['further_clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36f147d-6284-4ad8-ace3-1700c5632533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c341f5a4-be2d-4fbb-9120-2324cd1da2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "def getBagOfWords(text_list):\n",
    "    return set(text_list)\n",
    "\n",
    "documents = []\n",
    "for tokens in df['further_clean_text']:\n",
    "    bag = getBagOfWords(tokens)\n",
    "    num_of_words = dict.fromkeys(unique_words, 0)\n",
    "    for word in bag:\n",
    "        num_of_words[word] += 1\n",
    "    tf = computeTF(num_of_words, bag)\n",
    "    documents.append(tf)\n",
    "    \n",
    "idfs = computeIDF(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d1e94-d0ad-43a4-a164-3d5d9f16058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "dictionaries = []\n",
    "i = 0\n",
    "for tokens in df['further_clean_text']:\n",
    "    tfidf = computeTFIDF(documents[i], idfs)    \n",
    "    dictionaries.append(tfidf)\n",
    "    i += 1\n",
    "\n",
    "sample_df = pd.DataFrame.from_dict(dictionaries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82532625-a4ed-4d03-be7c-a1ee85a82d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alreadi</th>\n",
       "      <th>miss</th>\n",
       "      <th>sad</th>\n",
       "      <th>bulli</th>\n",
       "      <th>put</th>\n",
       "      <th>diego</th>\n",
       "      <th>son</th>\n",
       "      <th>go</th>\n",
       "      <th>leav</th>\n",
       "      <th>sooo</th>\n",
       "      <th>alon</th>\n",
       "      <th>bo</th>\n",
       "      <th>bought</th>\n",
       "      <th>interview</th>\n",
       "      <th>san</th>\n",
       "      <th>releas</th>\n",
       "      <th>id</th>\n",
       "      <th>couldnt</th>\n",
       "      <th>respond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.536479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804719</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804719</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alreadi      miss       sad     bulli      put     diego      son  \\\n",
       "0  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "1  0.00000  0.321888  0.321888  0.000000  0.00000  0.321888  0.00000   \n",
       "2  0.00000  0.000000  0.000000  0.804719  0.00000  0.000000  0.00000   \n",
       "3  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "4  0.26824  0.000000  0.000000  0.000000  0.26824  0.000000  0.26824   \n",
       "\n",
       "         go      leav      sooo      alon        bo   bought  interview  \\\n",
       "0  0.536479  0.000000  0.000000  0.000000  0.000000  0.00000   0.000000   \n",
       "1  0.000000  0.000000  0.321888  0.000000  0.000000  0.00000   0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.804719  0.00000   0.000000   \n",
       "3  0.000000  0.536479  0.000000  0.536479  0.000000  0.00000   0.536479   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.26824   0.000000   \n",
       "\n",
       "        san   releas        id  couldnt   respond  \n",
       "0  0.000000  0.00000  0.536479  0.00000  0.536479  \n",
       "1  0.321888  0.00000  0.000000  0.00000  0.000000  \n",
       "2  0.000000  0.00000  0.000000  0.00000  0.000000  \n",
       "3  0.000000  0.00000  0.000000  0.00000  0.000000  \n",
       "4  0.000000  0.26824  0.000000  0.26824  0.000000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbba66-0ad8-4601-8957-6f05c1a612d7",
   "metadata": {},
   "source": [
    "Due to memory limitations we will not be using TF-IDF for our embedding, but this demonstrates the issue adversarial examples create across social media. The vocabulary space has grown extremely large causing this approach to require more memory than we have access to. An industrial setting may have the hardware capable of supporting this approach, but the problem is also at a larger scale for industrial platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea2f0f-7631-4145-a418-f4d8f522cd63",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1672a3e-29c4-4502-995a-29fd0a31c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107cc20-b79f-4c7e-a0ad-2244c42226a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df['further_clean_text'], window=5, min_count=2, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2a51186-824d-418e-a56e-8cc15f6fcdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter', 0.9993133544921875),\n",
       " ('also', 0.9992782473564148),\n",
       " ('said', 0.9992430210113525),\n",
       " ('p', 0.9992425441741943),\n",
       " ('oh', 0.9992263913154602),\n",
       " ('whole', 0.9992219805717468),\n",
       " ('sweet', 0.999212920665741),\n",
       " ('almost', 0.9992028474807739),\n",
       " ('one', 0.9991977214813232),\n",
       " ('awesome', 0.999176025390625)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('hello', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e19eb5df-ea7c-488e-8442-f5adc9efb320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a908a11e-9d40-47a5-adb4-f80329dc05e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6519"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784bbc9-1504-4ce5-b7fa-14d4bb80aeda",
   "metadata": {},
   "source": [
    "So our dataset consists of 16044 documents each with varying number of words. Let us first try using the mean of the vectors of each words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85085f73-efb7-4924-965b-3b869f966842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(train_data, label_data):\n",
    "    n = 100\n",
    "    m = len(train_data)\n",
    "    X = np.zeros((m, n))\n",
    "    y = label_data\n",
    "\n",
    "    ind = 0\n",
    "    for tokens in train_data:\n",
    "        vec = np.zeros((n))\n",
    "        count = 0\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                count += 1\n",
    "                vec += model.wv[token]\n",
    "        if count != 0:\n",
    "            vec = np.divide(vec, count)\n",
    "        X[ind] = vec\n",
    "        ind += 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baedd20c-a969-4539-90f7-ac1bc0dfe632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13087,)\n",
      "(3272,)\n",
      "(13087,)\n",
      "(3272,)\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(df['further_clean_text'], df['classification'], test_size=0.2)\n",
    "print(X_train_raw.shape)\n",
    "print(X_test_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(y_test_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f15ae87d-629b-4629-b701-f7d0c9afc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_data(X_train_raw, y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76818c2c-8f70-4baf-aef7-6eed7a7db18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = create_data(X_test_raw, y_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5726e3-5b8f-4f48-9abd-c04c2accc158",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6aebc-34bd-4d62-bfca-260fe99b4bec",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75636958-db98-409d-9700-7b2e771976e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96f66c29-b218-4e48-a07d-50433006dddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=800, random_state=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_model = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "ada_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07eb3392-102d-4643-ac2a-b905bfcd7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7487775061124694\n",
      "0.7633851468048359\n"
     ]
    }
   ],
   "source": [
    "test_pred = ada_model.predict(X_test)\n",
    "print(accuracy_score(y_test, test_pred))\n",
    "print(f1_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8993d-e44b-4e98-95db-e82d8966696e",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87dc5bd0-5720-43d7-91d3-11312fcf457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Google Colab Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dac12-be3d-4b6c-8e5f-5a0540435c5d",
   "metadata": {},
   "source": [
    "### Sparse Structures Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a8e0255-facf-4338-8c25-fd29c18b7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337380c5-ed73-4935-be12-ed59ad9d8529",
   "metadata": {},
   "source": [
    "### Sparse Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f2ee62c-5c87-4817-970b-c4680c87627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1fb5da9-8c84-4013-b66a-714ef3d11ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=4000, penalty='l1', random_state=0, solver='saga')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='saga', penalty='l1', max_iter=4000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e7dd511-443e-4041-a314-3eeaf56dba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7652811735941321\n",
      "0.7734513274336284\n"
     ]
    }
   ],
   "source": [
    "clf_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, clf_pred))\n",
    "print(f1_score(y_test, clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a42d0dbf-ec88-4f71-b186-8aa9183b8bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=4000, penalty='none', random_state=0, solver='saga')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_orig = LogisticRegression(random_state=0, solver='saga', penalty='none', max_iter=4000)\n",
    "clf_orig.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d94cde9-61c8-4305-a545-f92370101465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777200488997555\n",
      "0.7865300146412885\n"
     ]
    }
   ],
   "source": [
    "clf_orig_pred = clf_orig.predict(X_test)\n",
    "print(accuracy_score(y_test, clf_orig_pred))\n",
    "print(f1_score(y_test, clf_orig_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318ff09-259c-4ef1-b3da-1508afb45a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
