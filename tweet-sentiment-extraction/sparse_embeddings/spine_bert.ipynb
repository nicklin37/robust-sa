{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5RAUBLKNpUUR"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank \n",
    "from nltk.tree import Tree\n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0A5wkPSq-R4",
    "outputId": "e405243c-8d8f-4774-dc92-8eeb0e43cb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69CC2_tXrF2I",
    "outputId": "fff7e1fd-38a6-48fc-f013-abf7cb68c87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial_swap_train.csv  adversarial_swap_train_final.csv\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/drive/My Drive/Colab Notebooks/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "qBU5d_KDpnHX",
    "outputId": "09481185-097e-4f3a-bd05-4eec24d84e97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>classification</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bos bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>son couldnt put release already bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>feeding baby fun smile coo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>journey wow u became cooler hehe possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>really really like song love story taylor swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sharpie running dangerously low ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>want go music tonight lost voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>uh oh sunburned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                       clean_text\n",
       "0           1  ...                          sooo sad miss san diego\n",
       "1           2  ...                                     bos bullying\n",
       "2           3  ...                            interview leave alone\n",
       "3           4  ...           son couldnt put release already bought\n",
       "4           6  ...                       feeding baby fun smile coo\n",
       "5           9  ...        journey wow u became cooler hehe possible\n",
       "6          11  ...  really really like song love story taylor swift\n",
       "7          12  ...              sharpie running dangerously low ink\n",
       "8          13  ...                 want go music tonight lost voice\n",
       "9          15  ...                                  uh oh sunburned\n",
       "\n",
       "[10 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/data/adversarial_swap_train_final.csv\")\n",
    "df = df.dropna()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ot69ysR_qmw9",
    "outputId": "70b46b61-9fef-42b9-fb08-cd964037a93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V0t_wpJiri_7"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7dD3kt9Urqjc"
   },
   "outputs": [],
   "source": [
    "def encode_for_bert(data, maximum_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "    for sentence in data:\n",
    "        if isinstance(sentence, float):\n",
    "          print(sentence)\n",
    "        encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maximum_length, pad_to_max_length=True, return_attention_mask=True, truncation=True)\n",
    "      \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6FufhfLrr-8",
    "outputId": "fa4e2570-6aaa-476d-b9fc-721ba78f7d5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(df['clean_text'], df['classification'], test_size=0.2)\n",
    "train_input_ids, train_attention_masks = encode_for_bert(X_train_raw, 23)\n",
    "test_input_ids, test_attention_masks = encode_for_bert(X_test_raw, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zpWrnkYNrtwz"
   },
   "outputs": [],
   "source": [
    "def add_sparse_layers(bert_model):\n",
    "    input_ids = tf.keras.Input(shape=(23,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(23,),dtype='int32')\n",
    "  \n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "    output = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer='l1')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTugPQz8ryBT",
    "outputId": "ed726c7d-86b4-4110-e05d-ce4a4a9a24a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4N3OoNQrzvU",
    "outputId": "1f481fd2-b0e7-4220-884e-674a16afddee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 23)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 23)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  335141888   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 23,                                                \n",
      "                                1024),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           32800       ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)           (None, 32)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            33          ['dropout_73[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335,174,721\n",
      "Trainable params: 335,174,721\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = add_sparse_layers(bert_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYe8V4r2sk0c",
    "outputId": "b595bb8f-9ac8-4b02-9065-80bf6337631e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1047/1047 [==============================] - 633s 567ms/step - loss: 12.0797 - accuracy: 0.8213 - val_loss: 11.2645 - val_accuracy: 0.8950\n",
      "Epoch 2/2\n",
      "1047/1047 [==============================] - 586s 559ms/step - loss: 10.5162 - accuracy: 0.9113 - val_loss: 9.8043 - val_accuracy: 0.9041\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_input_ids,train_attention_masks], y_train_raw, validation_split=0.2, epochs=2,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mGvw2O8BsoWT"
   },
   "outputs": [],
   "source": [
    "result = model.predict([test_input_ids,test_attention_masks])\n",
    "result = np.round(result).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uj9y0C1c_vfA"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo486pVpAopt",
    "outputId": "d8be93f0-54d4-449a-e8ba-2367a5de70ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8994498777506112\n",
      "0.9068780073591848\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test_raw, result))\n",
    "print(f1_score(y_test_raw, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "vNzVNQhio6AF"
   },
   "outputs": [],
   "source": [
    "def get_base_model(bert_model):\n",
    "    input_ids = tf.keras.Input(shape=(23,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(23,),dtype='int32')\n",
    "  \n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    output = tf.keras.layers.Dropout(0.4)(output)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAAdsS19uDgG",
    "outputId": "9b500c96-c706-498e-94d3-56ed6cbbaf42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 23)]         0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 23)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  335141888   ['input_5[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_6[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 23,                                                \n",
      "                                1024),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            1025        ['tf_bert_model[2][1]']          \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 1)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335,142,913\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "base_model = get_base_model(bert_model)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5QhRxLcuL10",
    "outputId": "2f704ee6-dde2-4d80-d719-811b11565184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1047/1047 [==============================] - 627s 560ms/step - loss: 3.2913 - accuracy: 0.7769 - val_loss: 0.4534 - val_accuracy: 0.8900\n",
      "Epoch 2/2\n",
      "1047/1047 [==============================] - 580s 554ms/step - loss: 3.3375 - accuracy: 0.7683 - val_loss: 0.3651 - val_accuracy: 0.8858\n"
     ]
    }
   ],
   "source": [
    "base_history = base_model.fit([train_input_ids,train_attention_masks], y_train_raw, validation_split=0.2, epochs=2,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OnHI4AOBuRUN"
   },
   "outputs": [],
   "source": [
    "base_result = base_model.predict([test_input_ids,test_attention_masks])\n",
    "base_result = np.round(result).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz0xjKZFzjRi"
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test_raw, base_result))\n",
    "print(f1_score(y_test_raw, base_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oqfU9-Tzk2Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sparse_bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
